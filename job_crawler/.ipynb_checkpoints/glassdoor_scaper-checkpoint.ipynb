{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from threading import Thread, Lock, Condition, Event\n",
    "from typing import List, Dict, Tuple, Sequence\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import signal\n",
    "import sys\n",
    "import platform\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "from collections import namedtuple\n",
    "import re\n",
    "from queue import Queue\n",
    "import threading\n",
    "import multiprocessing\n",
    "from multiprocessing import Manager, Process\n",
    "from MySQLWrapper import MySQLWrapper\n",
    "import MySQLdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glassdoor_main_url = \"https://www.glassdoor.com/Job/jobs.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=software&sc.keyword=software&locT=&locId=&jobType=\"\n",
    "time_limit = 12\n",
    "job_list_file = \"job_list.txt\"\n",
    "job_list = []\n",
    "def get_brower():\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument('--headless')  # 运行时关闭窗口\n",
    "    # 使用同一目录下的chromedriver进行模拟\n",
    "    browser_driver_address = str\n",
    "    if platform.system() == \"Windows\":\n",
    "        browser_driver_address = \"chromedriver.exe\"\n",
    "    elif platform.system() == \"Darwin\":\n",
    "        browser_driver_address = \"chromedriver_2.45_mac\"\n",
    "    elif platform.system() == \"Linux\":\n",
    "        browser_driver_address = \"chromedriver_2.36_ubuntu16.04\"\n",
    "    else:\n",
    "        browser_driver_address = \"chromedriver_2.36_ubuntu16.04\"\n",
    "\n",
    "    driver_path = os.path.join(os.path.abspath(\".\"), \"browser_drivers\", browser_driver_address)\n",
    "\n",
    "    print(\"chrome_drive:\", driver_path)\n",
    "    driver = webdriver.Chrome(driver_path, chrome_options=chrome_options)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_bunch_of_job(job_list: List[str], threadID: int):\n",
    "    print(threadID)\n",
    "    driver = get_brower()\n",
    "    for job in job_list:\n",
    "        crawl_one_job_title(job, driver)\n",
    "    driver.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_one_job_title(job: str, driver: webdriver.Chrome):\n",
    "    # go to the main page of glassdoor\n",
    "    driver.get(glassdoor_main_url)\n",
    "    driver.implicitly_wait(time_limit)\n",
    "    keyword = driver.find_element_by_id(\"sc.keyword\")\n",
    "    keyword.clear()\n",
    "    keyword.send_keys(job)\n",
    "    location = driver.find_element_by_id(\"sc.location\")\n",
    "    location.clear()\n",
    "    driver.find_element_by_id(\"HeroSearchButton\").click()\n",
    "    with MySQLWrapper() as conn:\n",
    "        for i in range(20):\n",
    "            try:\n",
    "                job_ul = driver.find_element_by_xpath(\"\"\"//*[@id=\"MainCol\"]/div/ul\"\"\")\n",
    "            except selenium.common.exceptions.NoSuchElementException as e:\n",
    "                # 如果没找到，那这个job很可能出错了，咱们跳过它\n",
    "                break\n",
    "\n",
    "            li_list = job_ul.find_elements_by_tag_name(\"li\")\n",
    "            for li in li_list:\n",
    "                # li.click() 有可能会失败，如果发生这种情况，是glassdoor网页崩了，应该重新跑这个任务\n",
    "                li.click()\n",
    "                driver.implicitly_wait(time_limit)\n",
    "\n",
    "                # 抓小广告，关了去\n",
    "                driver.implicitly_wait(0)  # 如果不设成0,会等time_limit时间的小广告\n",
    "                try:\n",
    "                    ad_div = driver.find_element_by_xpath(\"\"\"//div[@id=\"JAModal\"]/div\"\"\")\n",
    "                except selenium.common.exceptions.NoSuchElementException as e:\n",
    "                    ad_div = None\n",
    "\n",
    "                if ad_div:\n",
    "                    # 如果不是隐藏的广告，关闭之\n",
    "                    ad_div_class = ad_div.get_attribute(\"class\")\n",
    "                    if not ad_div_class.__contains__(\"hidden\"):\n",
    "                        ad_close_tab = driver.find_element_by_xpath(\"\"\"//*[@id=\"JAModal\"]/div/div[2]/div[1]\"\"\")\n",
    "                        ad_close_tab.click()\n",
    "\n",
    "                driver.implicitly_wait(time_limit)\n",
    "\n",
    "                job_id = li.get_attribute(\"data-id\")\n",
    "                job_title = li.get_attribute(\"data-normalize-job-title\")\n",
    "                employer_id = li.get_attribute(\"data-emp-id\")\n",
    "                job_location = li.get_attribute(\"data-job-loc\")\n",
    "                driver.implicitly_wait(time_limit)\n",
    "\n",
    "                # 如果不能获取公司名称，说明这个glassdoor网页有问题，这份工作跳过不管了\n",
    "                try:\n",
    "                    employer_wrapper_tag = driver.find_element_by_xpath(\n",
    "                        \"\"\"//*[@id=\"HeroHeaderModule\"]/div[contains(@class, 'empWrapper')]\"\"\")\n",
    "                except selenium.common.exceptions.StaleElementReferenceException as e:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    employer_wrapper_html = employer_wrapper_tag.get_attribute('innerHTML')\n",
    "                except selenium.common.exceptions.StaleElementReferenceException as e:\n",
    "                    continue\n",
    "                employer_wrapper_soup = BeautifulSoup(employer_wrapper_html, \"html.parser\")\n",
    "                company_name = employer_wrapper_soup.find('a', attrs={\"class\": \"empDetailsLink\"}).text\n",
    "                print(company_name)\n",
    "                # company_name_tag = driver.find_element_by_xpath(\n",
    "                #     \"\"\"//*[@id=\"HeroHeaderModule\"]/div[3]/div[3]/a\"\"\")\n",
    "                # # \"\"\"//*[@id=\"HeroHeaderModule\"]/div[3]/div[3]/a[contains(@class, 'empDetailsLink')]\"\"\"\n",
    "                # company_name = company_name_tag.text\n",
    "\n",
    "                # get the posted date of a job, if the job have the attribute, 找这个元素不需要等\n",
    "                driver.implicitly_wait(0)\n",
    "                try:\n",
    "                    # crawled post_time structure: n days ago\n",
    "                    post_time_tag = li.find_element_by_xpath(\"\"\".//span[contains(@class, 'minor')]\"\"\")\n",
    "                except selenium.common.exceptions.NoSuchElementException as e:\n",
    "                    post_time_tag = None\n",
    "\n",
    "                posted_time = None\n",
    "                if post_time_tag:\n",
    "                    # days_ago = post_time_tag.text.split()[0]\n",
    "                    # if days_ago == \"Today\":\n",
    "                    #     days_ago = 1\n",
    "                    # else:\n",
    "                    #     days_ago = int(days_ago)\n",
    "                    # posted_time = datetime.now() - timedelta(days=days_ago)\n",
    "                    posted_time = post_time_tag.text\n",
    "\n",
    "                driver.implicitly_wait(time_limit)\n",
    "                job_description_container = driver.find_element_by_xpath(\"\"\"//*[@id=\"JobDescriptionContainer\"]\"\"\")\n",
    "                description_div = job_description_container.find_element_by_class_name(\"jobDescriptionContent\")\n",
    "                job_description = description_div.text\n",
    "                job_description_html = description_div.get_attribute('innerHTML')\n",
    "                job_soup = BeautifulSoup(job_description_html, \"html.parser\")\n",
    "                job_description_html = job_soup.content\n",
    "\n",
    "                # 开始填充job_dict，准备插入到数据库\n",
    "                job_info_dict = {}\n",
    "                job_info_dict['job_id'] = job_id\n",
    "                job_info_dict['job_title'] = job_title\n",
    "                job_info_dict['company_id'] = employer_id\n",
    "                job_info_dict['location'] = job_location\n",
    "                job_info_dict['job_description'] = job_description\n",
    "                job_info_dict['job_description_html'] = job_description_html\n",
    "                if posted_time:\n",
    "                    job_info_dict['posted_time'] = posted_time\n",
    "\n",
    "                # crawl its company infomation\n",
    "                company_info_dict = dict()\n",
    "                company_info_dict[\"company_id\"] = employer_id\n",
    "                company_info_dict[\"company_name\"] = company_name\n",
    "                company_website_url = None\n",
    "\n",
    "                driver.implicitly_wait(1)  # this should be quick\n",
    "                try:\n",
    "                    company_tag = driver.find_element_by_xpath(\n",
    "                        \"\"\"//*[@id=\"Details\"]/div[2]/header//div[contains(@class, 'scrollableTabs')]//span[contains(text(), 'Company')]\"\"\")\n",
    "                except selenium.common.exceptions.NoSuchElementException as e:\n",
    "                    company_tag = None\n",
    "\n",
    "                if company_tag:\n",
    "                    company_tag.click()\n",
    "                    basic_info_div = driver.find_element_by_xpath(\"\"\"//*[@id=\"EmpBasicInfo\"]\"\"\")\n",
    "                    info_entities = basic_info_div.find_elements_by_xpath(\n",
    "                        \"\"\"//div[contains(@class, 'infoEntity')]\"\"\")\n",
    "                    for entity in info_entities:\n",
    "                        label = entity.find_element_by_tag_name(\"label\")\n",
    "                        span = entity.find_element_by_tag_name(\"span\")\n",
    "                        key = label.text\n",
    "                        value = span.text\n",
    "                        company_info_dict[key] = value\n",
    "                    try:\n",
    "                        company_website_tag = basic_info_div.find_element_by_xpath(\".//a[contains(text(), 'Visit']\")\n",
    "                    except selenium.common.exceptions.NoSuchElementException as e:\n",
    "                        company_website_tag = None\n",
    "                    if company_website_tag:\n",
    "                        company_website_url = company_website_tag.get_attribute(\"href\")\n",
    "                        company_info_dict[\"website\"] = company_website_url\n",
    "\n",
    "                # try:\n",
    "                #     salary_tag = driver.find_element_by_xpath(\n",
    "                #         \"\"\"//*[@id=\"Details\"]/div[2]/header//div[contains(@class, 'scrollableTabs')]//span/[contains(text(), 'Salary')]\"\"\")\n",
    "                # except selenium.common.exceptions.NoSuchElementException as e:\n",
    "                #     salary_tag = None\n",
    "                # if salary_tag:\n",
    "                #     salary_tag.click()\n",
    "                #     driver.implicitly_wait(1)\n",
    "                # print(company_info)\n",
    "\n",
    "                # 將爬取到的company信息和job信息存入数据库\n",
    "\n",
    "                conn.insert_one_row_by_dict(\"company_data_unclean\", company_info_dict)\n",
    "\n",
    "                # 将job信息插入数据库,错了就跳过这个job，不要了\n",
    "                try:\n",
    "                    conn.insert_one_row_by_dict(\"job_data_unclean\", job_info_dict)\n",
    "                    print(\"成功插入一条job\", job_info_dict['job_id'], job_info_dict['job_tile'])\n",
    "                except MySQLdb._exceptions.OperationalError as e:\n",
    "                    print(\"已经存在\", job_info_dict)\n",
    "                    continue\n",
    "\n",
    "            # 如果有下一页 我们就翻到下一页去\n",
    "            try:\n",
    "                next_page = driver.find_element_by_xpath(\"\"\"//*[@id=\"FooterPageNav\"]/div/ul/li[7]/a\"\"\")\n",
    "            except selenium.common.exceptions.NoSuchElementException as e:\n",
    "                next_page = None  # this mean we do not have next_page\n",
    "            if next_page:\n",
    "                next_page.click()\n",
    "                driver.implicitly_wait(time_limit)\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "chrome_drive: /home/abel/PycharmProjects/Shortest_job_first/job_crawler/browser_drivers/chromedriver_2.36_ubuntu16.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abel/anaconda3/envs/pycharm/lib/python3.6/site-packages/ipykernel/__main__.py:22: DeprecationWarning: use options instead of chrome_options\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with open(job_list_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [x.strip() for x in lines]\n",
    "        for line in lines:\n",
    "            job_list.append(line)\n",
    "            line = f.readline()\n",
    "\n",
    "    pool_size = 1\n",
    "    pool = multiprocessing.Pool(pool_size)\n",
    "    total_num_job_titles = len(job_list)\n",
    "    one_thread_task = total_num_job_titles // 4\n",
    "\n",
    "    job_divisions = []\n",
    "    for i in range(pool_size):\n",
    "        if i == 0:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = i * one_thread_task\n",
    "\n",
    "        if i == pool_size:\n",
    "            end = one_thread_task\n",
    "        else:\n",
    "            end = (i + 1) * one_thread_task\n",
    "\n",
    "        division = job_list[int(start):int(end)]\n",
    "        job_divisions.append(division)\n",
    "\n",
    "    processes = []\n",
    "    for i in range(pool_size):\n",
    "        pro = Process(target=crawl_bunch_of_job, args=(job_divisions[i], i), daemon=False)\n",
    "        processes.append(pro)\n",
    "        pro.start()\n",
    "\n",
    "    for i in range(pool_size):\n",
    "        processes[i].join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pycharm]",
   "language": "python",
   "name": "conda-env-pycharm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
